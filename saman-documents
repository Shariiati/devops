---------------------------------------------------------------

Permanent Resolv.Conf
=====================
sudo apt install resolvconf -y
sudo systemctl status resolvconf.service
sudo systemctl enable resolvconf.service
sudo vim /etc/resolvconf/resolv.conf.d/head
sudo systemctl restart resolvconf.service
sudo systemctl restart systemd-resolved.service
sudo resolvconf --enable-updates
sudo resolvconf -u 
 
nameserver 178.22.122.100
nameserver 185.51.200.2
------------------------------------------------------------------

Nexus_password

sudo vim /etc/apt/auth.conf.d/pr.si24.ir.conf

machine https://pr.si24.ir:8443/
login deployer
password D3@p1Io#0%oy!3R
----------------------------------------------------------------
daemon.json mirror repository
 
{
   "insecure-registries": [
      "http://192.168.100.198:9000",
      "http://192.168.100.198:9001",
      "http://192.168.100.198:9002",
      "http://192.168.100.198:9003"
   ],

	"registry-mirrors":
	[
	"https://docker.nowire.ir",
	"https://docker.iranrepo.ir",
	"https://docker.host:5000",
	"https://registry.docker.ir",
	"https://docker.iranserver.com"
	]
} 
 

--------------------------------------------------------------
netplan config:::

network:
  ethernets:
    ens33:
      dhcp4: false
      addresses:
        - 10.1.25.120/24
      gateway4: 10.1.25.1
      nameservers:
        addresses:
          - 192.168.100.6
          - 192.168.100.7
          - 8.8.8.8
  version: 2
-------------------------------------------------------------
delete ns stuck in terminating:


NS=`kubectl get ns |grep Terminating | awk 'NR==1 {print $1}'` && kubectl get namespace "$NS" -o json   | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/"   | kubectl replace --raw /api/v1/namespaces/$NS/finalize -f -

----------------------------------------------------------------
create secret for nexus-image repository:

kubectl create secret -n test-app docker-registry nexus-cred --docker-server=192.168.100.198:9003 --docker-username=deployer --docker-password='D3@p1Io#0%oy!3R'

----------------------------------------------------------------

sudo gitlab-runner register \
  --non-interactive \
  --url "https://cr.si24.ir/" \
  --tls-ca-file /etc/gitlab-runner/ssl/cr.si24.ir.crt \
  --registration-token "GR1348941NzM8z3j-spTUV5FJP6YZ" \
  --description "Developing Life PolicyData " \
  --maintenance-note "Life_PolicyData Maintenance Mode. Please wait..." \
  --executor ssh \
  --ssh-host 127.0.0.1 \
  --ssh-port 2302 \
  --ssh-user deployer \
  --ssh-password D3@p1Io#0%oy$3R \
  --ssh-identity-file "/home/deployer/.ssh/id_rsa" \
  --shell bash \
  --run-untagged="false" \
  --tag-list "develop, dev-life-pdata" \
  --locked="true" \
  --ssh-disable-strict-host-key-checking="true" \
  --config /etc/gitlab-runner/config.toml

----------------------------------------------------------------
removing docker containers and remove them
docker stop $(docker ps -aq) && docker rm $(docker ps -aq)  "&& docker rmi $(docker images -q)"
-----------------------------------------------------------------
enable argocd exec shell

1- edit configmap argocd-cm to and enable exec:
apiVersion: v1
data:
  accounts.test: apiKey,login
  admin.enabled: "true"
  application.instanceLabelKey: argocd.argoproj.io/instance
  exec.enabled: "true"  **
  server.insecure: "true"
  server.rbac.log.enforce.enable: "false"
  timeout.hard.reconciliation: 0s
  timeout.reconciliation: 180s
kind: ConfigMap

2- edit clusterrole and role argocd-server and enable the exec:

- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - pods/exec  **
  verbs:
  - get
  - create  **

3- edit the rbac cm argocd-rbac-cm add the line:
apiVersion: v1
data:
  policy.csv: |
**  p, role:testgrp, exec, create, */*, allow
    p, role:testgrp, applications, get, *, allow
**  p, role:admin, exec, create, */*, allow
    g, admin, role:admin
    g, test, role:testgrp
  scopes: '[groups]'
kind: ConfigMap

[‎11/‎20/‎2023 1:51 PM]  
https://www.youtube.com/watch?v=VJUWqegYXVE 
 
------------------------------------------------------------

adding new disk to ubuntu for mounting to a directory

for host in /sys/class/scsi_host/*; do echo "- - -" | sudo tee $host/scan; ls /dev/sd* ; done

1-       parted /dev/sdb
(parted) mklabel gpt                     # create a GPT partition table
(parted) mkpart primary ext4 0% 100%     # create a primary partition using all available space
(parted) quit

2-  pvcreate /dev/sdb1
    vgcreate ubuntu-vg2 /dev/sdb1

3-  lvcreate -n ubuntu-lv2 -L 95G(-l +100%FREE ) ubuntu-vg2 

4-  mkfs.ext4 /dev/ubuntu-vg2/ubuntu-lv2

5-  mkdir /mnt/hdd
    mount /dev/ubuntu-vg2/ubuntu-lv2 /mnt/hdd

6-  vi /etc/fstab
   /dev/ubuntu-vg2/ubuntu-lv2 /mnt/hdd ext4 defaults 0 0
7- resize2fs /dev/ubuntu-vg2/ubuntu-lv2
--------------------------------------------------------
extend disk :

1-echo 1 > /sys/class/block/sda/device/rescan
2- cfdisk add to partition
3-pvresize /dev/sda3
4-lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv
5-resize2fs /dev/ubuntu-vg/ubuntu-lv
---------------------------------------------------------
taint and tolerations for node_exporter daemonset:
## add these to daemonset that you want to schedule on masternodes also
    tolerations:
    - key: "node-role.kubernetes.io/etcd"
      operator: "Exists"
      effect: "NoExecute"
    - key: "node-role.kubernetes.io/controlplane"
      operator: "Exists"
      effect: "NoSchedule"
---------------------------------------------------------
see the logs of failing container:
kubectl logs --follow pod/cattle-cluster-agent-76866dbc87-s2kzb -n cattle-system
---------------------------------------------------------
adding an entry to coredns A record:

kubectl get configmap -n kube-system coredns -o yaml > coredns-config.yaml

apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
        hosts {                                    **
            rancher.si24.ir 192.168.110.70         **
        }                                          **
    }
--------------------------------------------------------------
##############################Grafana Ldap Authentication:################################################

1-create a ldap-toml file with these context:
[[servers]]
# Ldap server host (specify multiple hosts space separated)
# We use OpenLDAP but you can use any IP address or external host here
host = "192.168.100.6"
# Default port is 389 or 636 if use_ssl = true
port = 389
# Set to true if LDAP server supports TLS
use_ssl = false
# Set to true if connect LDAP server with STARTTLS pattern (create connection in insecure, then upgrade to secure connection with TLS)
start_tls = false
# set to true if you want to skip SSL cert validation
ssl_skip_verify = true

# Search user bind dn
bind_dn = "CN=bind,OU=System-Account,DC=saman,DC=com"
# Search user bind password
bind_password = "Robot@1234"
# User search filter, for example "(cn=%s)" or "(sAMAccountName=%s)" or "(uid=%s)"
search_filter = "(sAMAccountName=%s)"
# An array of base dns to search through
search_base_dns = ["DC=saman,DC=com"]
# Specify names of the LDAP attributes your LDAP uses
[servers.attributes]
name = "givenName"
surname = "sn"
username = "cn"
member_of = "memberOf"
email =  "email"

2- next create a secret file named grafana-ldap-toml :
   kubectl -n monitoring create secret generic grafana-ldap-toml --from-file=ldap-toml
 
3- in helmchart head to the ./charts/grafana and edit the values.yaml file and find the grafana.ini :
 make these changes
a.  auth.ldap:
    enabled: true
    allow_sign_up: true

b.  ldap:
    enabled: true
    existingSecret: "grafana-ldap-toml"

grafana.ini:
  paths:
    data: /var/lib/grafana/
    logs: /var/log/grafana
    plugins: /var/lib/grafana/plugins
    provisioning: /etc/grafana/provisioning
  analytics:
    check_for_updates: true
  log:
    mode: console
  grafana_net:
    url: https://grafana.net
  server:
    domain: "{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ .Values.ingress.hosts | first }}{{ else }}''{{ end }}"
## grafana Authentication can be enabled with the following values on grafana.ini
 # server:
      # The full public facing url you use in browser, used for redirects and emails
 #    root_url:
 # https://grafana.com/docs/grafana/latest/auth/github/#enable-github-in-grafana
 #  auth.github:
 #     enabled: false
 #     allow_sign_up: true
 #    scopes: user:email,read:org
 #    auth_url: https://github.com/login/oauth/authorize
 #    token_url: https://github.com/login/oauth/access_token
 #    api_url: https://api.github.com/user
 #    team_ids:
 #    allowed_organizations:
 #    client_id:
 #    client_secret:
## LDAP Authentication can be enabled with the following values on grafana.ini
## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
  auth.ldap:
    enabled: true
    allow_sign_up: true
  #   config_file: /etc/grafana/ldap.toml

## Grafana's LDAP configuration
## Templated by the template in _helpers.tpl
## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
## ref: http://docs.grafana.org/installation/ldap/#configuration
ldap:
  enabled: true
  # `existingSecret` is a reference to an existing secret containing the ldap configuration
  # for Grafana in a key `ldap-toml`.
  existingSecret: "grafana-ldap-toml"
  # `config` is the content of `ldap.toml` that will be stored in the created secret
  config: ""
  # config: |-
  #   verbose_logging = true

  #   [[servers]]
  #   host = "my-ldap-server"
  #   port = 636
  #   use_ssl = true
  #   start_tls = false
  #   ssl_skip_verify = false

in the end use helm upgrade -n monitoring kube-prometheus-stack -f values-prod.yaml .
if the new pods get stuck just delete the previous ones so they let go of the pvc and new ones can get the volumes

-----------------------------------------------------------------------------------------------------
